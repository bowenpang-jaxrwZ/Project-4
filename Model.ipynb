{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>...</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.5</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>45</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
       "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
       "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
       "3  7795-CFOCW    Male              0      No         No      45           No   \n",
       "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
       "\n",
       "      MultipleLines InternetService OnlineSecurity  ... DeviceProtection  \\\n",
       "0  No phone service             DSL             No  ...               No   \n",
       "1                No             DSL            Yes  ...              Yes   \n",
       "2                No             DSL            Yes  ...               No   \n",
       "3  No phone service             DSL            Yes  ...              Yes   \n",
       "4                No     Fiber optic             No  ...               No   \n",
       "\n",
       "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
       "0          No          No              No  Month-to-month              Yes   \n",
       "1          No          No              No        One year               No   \n",
       "2          No          No              No  Month-to-month              Yes   \n",
       "3         Yes          No              No        One year               No   \n",
       "4          No          No              No  Month-to-month              Yes   \n",
       "\n",
       "               PaymentMethod MonthlyCharges  TotalCharges Churn  \n",
       "0           Electronic check          29.85         29.85    No  \n",
       "1               Mailed check          56.95        1889.5    No  \n",
       "2               Mailed check          53.85        108.15   Yes  \n",
       "3  Bank transfer (automatic)          42.30       1840.75    No  \n",
       "4           Electronic check          70.70        151.65   Yes  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Resources/Telco-Customer-Churn.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### refer to https://sparkbyexamples.com/pandas/pandas-dataframe-query-examples\n",
    "\n",
    "for col in df.columns.to_list():\n",
    "    value = ' '\n",
    "    query_string = f\"{col} == @value\"\n",
    "#     print(query_string)\n",
    "\n",
    "    if df.query(query_string)[col].count() > 0: \n",
    "        print(df.query(query_string)[col].count())\n",
    "        print(df.query(query_string))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing the TotalCharges column.  There were no nulls but there were empty values preventing conversion to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty strings with 0\n",
    "df['TotalCharges'] = df['TotalCharges'].replace(' ', 0)\n",
    "\n",
    "# Convert the column to float\n",
    "df['TotalCharges'] = df['TotalCharges'].astype(float)\n",
    "\n",
    "# Replace 0 values with the mean value\n",
    "mean_value = df['TotalCharges'].mean()\n",
    "df['TotalCharges'] = df['TotalCharges'].replace(0, mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='customerID',inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_churn(df, feature, target='Churn'):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.countplot(data=df, x=feature, hue=target)\n",
    "    plt.title(f'Churn by {feature}', fontsize=15)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender does not seem play a role in churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function\n",
    "plot_churn(df, 'gender')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers without a partner are more likely to churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_churn(df,'Partner')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers with no dependents are more likely to churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_churn(df,'Dependents')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Month-to-month contracts are MUCH more likely to churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_churn(df,'Contract')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers without device protection are more likely to churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_churn(df,'DeviceProtection')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers with paperless billing are more likely to churn, but also double the amount of representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_churn(df,'PaperlessBilling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PaperlessBilling'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_churn(df,'InternetService')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['InternetService'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is imbalanced, so we'll attempt to balance it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Churn', axis=1)  \n",
    "y = df['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resampled.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train[['MonthlyCharges','TotalCharges']])\n",
    "X_train[['MonthlyCharges','TotalCharges']] = scaler.transform(X_train[['MonthlyCharges','TotalCharges']])\n",
    "X_test[['MonthlyCharges','TotalCharges']] = scaler.transform(X_test[['MonthlyCharges','TotalCharges']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['gender', \n",
    "              'SeniorCitizen', \n",
    "              'Partner', \n",
    "              'Dependents',\n",
    "              'PhoneService', \n",
    "              'MultipleLines', \n",
    "              'InternetService', \n",
    "              'OnlineSecurity',\n",
    "              'OnlineBackup', \n",
    "              'DeviceProtection', \n",
    "              'TechSupport', \n",
    "              'StreamingTV',\n",
    "              'StreamingMovies', \n",
    "              'Contract', \n",
    "              'PaperlessBilling', \n",
    "              'PaymentMethod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_train_data = X_train[categories]\n",
    "encoder.fit(categorical_train_data)\n",
    "encoded_train_data = encoder.transform(categorical_train_data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data\n",
    "categorical_test_data = X_test[categories]\n",
    "encoded_test_data = encoder.transform(categorical_test_data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_df = pd.DataFrame(encoded_train_data, columns=encoder.get_feature_names_out(categories), index=X_train.index)\n",
    "encoded_test_df = pd.DataFrame(encoded_test_data, columns=encoder.get_feature_names_out(categories), index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the original dataframes with the new ones\n",
    "X_train_encoded = pd.concat([X_train.drop(categories, axis=1), encoded_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test.drop(categories, axis=1), encoded_test_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000)),\n",
    "    ('Support Vector Machine', SVC()),\n",
    "    ('Random Forest', RandomForestClassifier())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(models, X_train, y_train, X_test, y_test):\n",
    "    for name, model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        print(f'{name} Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_models(models, X_train_encoded, y_train, X_test_encoded, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def best_parameters(model, params, X, y, cv=5):\n",
    "    grid = GridSearchCV(model, params, cv=cv)\n",
    "    grid.fit(X, y)\n",
    "    print(\"Best parameters for \", str(model), \" are \", grid.best_params_)\n",
    "    print(\"Best score for \", str(model), \" is \", grid.best_score_)\n",
    "    return grid.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression tuning\n",
    "- The tuned model performed worse than the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and parameters\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], \n",
    "          'penalty': ['l1', 'l2'],\n",
    "          'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "# Get best parameters\n",
    "best_LR_params = best_parameters(model, params, X_train_encoded, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC Tuning\n",
    "- I tried multiple different parameters and let it run for an hour each time but it never finished.\n",
    "- After some research, I found out this model is known for having a long training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from scipy.stats import uniform\n",
    "\n",
    "# model = SVC()\n",
    "# # Specifying the parameters. The difference here is that we provide a distribution for continuous parameters rather than a list of specific values.\n",
    "# params = {'C': uniform(loc=0, scale=1000), \n",
    "#           'gamma': uniform(loc=0, scale=1),\n",
    "#           'kernel': ['linear', 'rbf']}\n",
    "\n",
    "# # Instantiate the RandomizedSearchCV object\n",
    "# rscv = RandomizedSearchCV(model, params, n_iter=100, cv=5, random_state=42)\n",
    "\n",
    "# # This will start the search over the specified parameter distributions\n",
    "# rscv.fit(X_train_encoded, y_train)\n",
    "\n",
    "# # Get the best parameters and the best score\n",
    "# best_svc_params = rscv.best_params_\n",
    "# best_svc_score = rscv.best_score_\n",
    "\n",
    "# print(\"Best parameters for SVC are \", best_svc_params)\n",
    "# print(\"Best score for SVC is \", best_svc_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "params = {\n",
    "    'n_estimators': [100],  # 100 is a good general starting point\n",
    "    'max_features': ['auto', 'sqrt', 0.5],  # Lower values can help to decrease complexity\n",
    "    'max_depth': [10, 20, 30],  # Lower values can also decrease complexity\n",
    "    'min_samples_split': [5, 10, 20],  # Higher values lead to more regularization\n",
    "    'min_samples_leaf': [5, 10, 20],  # Higher values also lead to more regularization\n",
    "    'bootstrap': [True]  # True can lead to a more diverse set of trees\n",
    "}\n",
    "\n",
    "# Get best parameters\n",
    "best_forest_params = best_parameters(model, params, X_train_encoded, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with updated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(bootstrap=False, max_depth=30, max_features='auto', \n",
    "                               min_samples_leaf=1, min_samples_split=2, n_estimators=10)\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Make predictions\n",
    "train_preds = model.predict(X_train_encoded)\n",
    "test_preds = model.predict(X_test_encoded)\n",
    "\n",
    "# Print classification report for training data\n",
    "print(\"Training Classification Report:\")\n",
    "print(classification_report(y_train, train_preds))\n",
    "\n",
    "# Print classification report for test data\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, test_preds))\n",
    "\n",
    "# Print confusion matrices\n",
    "print(\"Training Confusion Matrix:\")\n",
    "print(confusion_matrix(y_train, train_preds))\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, test_preds))\n",
    "\n",
    "# Print AUC-ROC scores\n",
    "print(\"Training AUC-ROC Score:\")\n",
    "print(roc_auc_score(y_train, model.predict_proba(X_train_encoded)[:, 1]))\n",
    "print(\"Test AUC-ROC Score:\")\n",
    "print(roc_auc_score(y_test, model.predict_proba(X_test_encoded)[:, 1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000,C= 1, penalty= 'l2', solver= 'sag')\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Make predictions\n",
    "train_preds = model.predict(X_train_encoded)\n",
    "test_preds = model.predict(X_test_encoded)\n",
    "\n",
    "# Print classification report for training data\n",
    "print(\"Training Classification Report:\")\n",
    "print(classification_report(y_train, train_preds))\n",
    "\n",
    "# Print classification report for test data\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, test_preds))\n",
    "\n",
    "# Print confusion matrices\n",
    "print(\"Training Confusion Matrix:\")\n",
    "print(confusion_matrix(y_train, train_preds))\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, test_preds))\n",
    "\n",
    "# Print AUC-ROC scores\n",
    "print(\"Training AUC-ROC Score:\")\n",
    "print(roc_auc_score(y_train, model.predict_proba(X_train_encoded)[:, 1]))\n",
    "print(\"Test AUC-ROC Score:\")\n",
    "print(roc_auc_score(y_test, model.predict_proba(X_test_encoded)[:, 1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "- Tenure was not scaled for the previous models, these tests will scale tenure with the other numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on the training data\n",
    "scaler.fit(X_train_encoded[['tenure']])\n",
    "\n",
    "# Transform the 'Tenure' column in both the training and test sets\n",
    "X_train_encoded['tenure'] = scaler.transform(X_train_encoded[['tenure']])\n",
    "X_test_encoded['tenure'] = scaler.transform(X_test_encoded[['tenure']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the base models with the scaled 'tenure' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_models(models, X_train_encoded, y_train, X_test_encoded, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with optimized models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest\n",
    "- As nice as a perfect model looks, these parameters might be overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(bootstrap=False, max_depth=30, max_features='auto', \n",
    "                               min_samples_leaf=1, min_samples_split=2, n_estimators=10)\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Make predictions\n",
    "train_preds = model.predict(X_train_encoded)\n",
    "test_preds = model.predict(X_test_encoded)\n",
    "\n",
    "# Print classification report for training data\n",
    "print(\"Training Classification Report:\")\n",
    "print(classification_report(y_train, train_preds))\n",
    "\n",
    "# Print classification report for test data\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, test_preds))\n",
    "\n",
    "# Print confusion matrices\n",
    "print(\"Training Confusion Matrix:\")\n",
    "print(confusion_matrix(y_train, train_preds))\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, test_preds))\n",
    "\n",
    "# Print AUC-ROC scores\n",
    "print(\"Training AUC-ROC Score:\")\n",
    "print(roc_auc_score(y_train, model.predict_proba(X_train_encoded)[:, 1]))\n",
    "print(\"Test AUC-ROC Score:\")\n",
    "print(roc_auc_score(y_test, model.predict_proba(X_test_encoded)[:, 1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model with new params = 'bootstrap': True, 'max_depth': 20, 'max_features': 0.5, 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 100\n",
    "model = RandomForestClassifier(bootstrap=True, max_depth=20, max_features=0.5, \n",
    "                               min_samples_leaf=5, min_samples_split=10, n_estimators=100)\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Make predictions\n",
    "train_preds = model.predict(X_train_encoded)\n",
    "test_preds = model.predict(X_test_encoded)\n",
    "\n",
    "# Print classification report for training data\n",
    "print(\"Training Classification Report:\")\n",
    "print(classification_report(y_train, train_preds))\n",
    "\n",
    "# Print classification report for test data\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, test_preds))\n",
    "\n",
    "# Print confusion matrices\n",
    "print(\"Training Confusion Matrix:\")\n",
    "print(confusion_matrix(y_train, train_preds))\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, test_preds))\n",
    "\n",
    "# Print AUC-ROC scores\n",
    "print(\"Training AUC-ROC Score:\")\n",
    "print(roc_auc_score(y_train, model.predict_proba(X_train_encoded)[:, 1]))\n",
    "print(\"Test AUC-ROC Score:\")\n",
    "print(roc_auc_score(y_test, model.predict_proba(X_test_encoded)[:, 1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000,C= 1, penalty= 'l2', solver= 'sag')\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Make predictions\n",
    "train_preds = model.predict(X_train_encoded)\n",
    "test_preds = model.predict(X_test_encoded)\n",
    "\n",
    "# Print classification report for training data\n",
    "print(\"Training Classification Report:\")\n",
    "print(classification_report(y_train, train_preds))\n",
    "\n",
    "# Print classification report for test data\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, test_preds))\n",
    "\n",
    "# Print confusion matrices\n",
    "print(\"Training Confusion Matrix:\")\n",
    "print(confusion_matrix(y_train, train_preds))\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, test_preds))\n",
    "\n",
    "# Print AUC-ROC scores\n",
    "print(\"Training AUC-ROC Score:\")\n",
    "print(roc_auc_score(y_train, model.predict_proba(X_train_encoded)[:, 1]))\n",
    "print(\"Test AUC-ROC Score:\")\n",
    "print(roc_auc_score(y_test, model.predict_proba(X_test_encoded)[:, 1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# classifier = LogisticRegression(solver='lbfgs', random_state=1,max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score isn't bad, but let's see if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Score the model\n",
    "# print(f\"Training Data Score: {classifier.score(X_train_encoded, y_train)}\")\n",
    "# print(f\"Testing Data Score: {classifier.score(X_test_encoded, y_test)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
